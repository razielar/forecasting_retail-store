\section[Main objective]{Main objective}
\label{sec:main_objective}

The \textbf{main object} of the project is to:

\begin{itemize}
\item Generate a reliable \textbf{time series forecast} based on sales from Favorita stores. 
\end{itemize}

Favorita stores is a  grocery retail store based in Ecuador. Producing an accurate forecast could lead to decreased food waste related to overstocking and improve customer satisfaction. All the code can be found in the following \href{https://github.com/razielar/forecasting_retail-store}{GitHub repository}, further information can be found in \nameref{sec:supp_info}.

\clearpage

\section[Data description]{Data description}
\label{sec:data_description}

The dataset comes from \textbf{Kaggle}, named: \href{https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data}{Store Sales - Time Series Forecasting}. The Kaggle dataset contains sales data from \textit{Corporaci√≥n Favorita}, a large Ecuadorian-based grocery retailer. Kaggle provides you with 7 diffeent files (see \autoref{tab:files} for more details). 

\begin{table}[!htb]
  \caption[Kaggle files description]{\textbf{Kaggle files description}. Files are alphabetically sorted.}
  \begin{scriptsize}
    \begin{tabulary}{0.65\linewidth}{ccl}
      \textbf{Number} & \textbf{File Name} & \textbf{Description} \\ \hline
      1 & holiday\_events.csv & Relevant holidays in Ecuador  \\
      2 & oil.csv & Oil prices from 2013 to 2017  \\
      3 & sample\_submission.csv & submission example  \\
      4 & stores.csv & Stores metadata  \\
      5 & test.csv & Stores and family products  \\
      6 & train.csv & Sales by store and product-family from 2013-01 to 2017-08  \\
      7 & transactions.csv & Number of transactions by store  \\
    \end{tabulary}
  \end{scriptsize}
  \label{tab:files}
\end{table}

The training data represents 99\% of the data, including dates from 2013-01-01 to 2017-08-16 (55.5 months), 54 stores placed in different cities within Ecuador, and 33 family-products (see \autoref{fig:store_summary}). The testing data includes dates from 2017-08-16 to 2017-08-31 (15 days).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/data_description/stores_summary.png}
  \caption[Summary of the training dataset]{\textbf{Summary of the training dataset}. The cluster information denotes similarity between stores.}
  \label{fig:store_summary}
\end{figure}

\section[Data exploration and data cleaning]{Data exploration and data cleaning}
\label{sec:eda}

Performing an exploratory data analysis (eda) of the sales from the training dataset, we can observe that grocery I, beverages, and produce are the top 3 most consumed products (see \autoref{fig:eda}) . Additionally, store type A, and cluster 5 are the most frequent among their classification (\autoref{fig:eda}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/sale_analysis.png}
  \caption[EDA of sales]{\textbf{EDA of sales}. The plot describes the sales by product, store type, and per cluster (left, right, and below, respectively). Darker blue represents higher sales. }
  \label{fig:eda}
\end{figure}


The sales represented by \autoref{fig:target} shows low-peaks at the end of each year, which is explained because the stores are closed at New years. Moreover, we can observe a pattern at each year, with increased sales at the end of the year which overlaps with the Christmas eves, suggesting a seasonal pattern. These patterns are highlighted after smoothing the sales data  with a 7 days moving average (the continuous red lines from the \autoref{fig:target}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/target-time-series-plot-DF-testProc.png}
  \caption[Target time series data]{\textbf{Target time series data}. The y-axis represents the training sales from 2013-01-01 to 2017-08-15, and the x-axis shows the time in days (1,087 days). The sales were aggregated by all stores and products (see \autoref{fig:store_summary}). The raw data, and smoothed data (7 days moving average) are denoted by the continuous blue, and red lines, respectively.}
  \label{fig:target}
\end{figure}

\subsection[Time series Decomposition]{Time series Decomposition}
\label{sec:decomposition}

Time series are defined as a sequence of data organized in time order. The key components of time-series are: trend, seasonality, and residuals, displayed in \autoref{fig:ts-decomposition}B, \autoref{fig:ts-decomposition}C, and \autoref{fig:ts-decomposition}D, respectively. Trend is the long-term direction of the time series, seasonality describes the periodic behavior such as holidays, and residuals represent the irregular fluctuations that we are not able to predict using the trend and/or seasonality. 

In our target time series, we observe an upward trend, with a clear seasonality factor (this will be further analyzed below), and the residuals do not follow a random pattern. These insights will be used to select an adequate forecast model. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.95\textwidth]{plots/eda/timseries_decomposition.png}
  \caption[Time series decomposition]{\textbf{Time series decomposition}. \textbf{(A)} Raw sales time series (sames as \autoref{fig:target}). \textbf{(B)} Trend of sales. \textbf{(C)} Seasonality of sales. \textbf{(D)} Residuals of sales. }
  \label{fig:ts-decomposition}
\end{figure}

\subsection[Stationarity and seasonality analyses]{Stationarity and seasonality analyses}
\label{sec:st_s_analysis}


The stationary and seasonality are relevant components to select the adequate machine learning model (such as ARIMA, SARIMA, etc.) to generate a reliable forecast. A stationary time series is defined, if their statistical properties such as mean, and variance are all constant, and independent of time.

In consequence, we implemented the \textit{Dickey-Fuller test} to assess stationarity. We obtained a \textit{p-value} of 0.09, using a significance level of 0.05, we can reject the null hypothesis (the series is stationary) concluding that our series is non-stationarity.

In terms of seasonality, we aggregate the sales for each month and we can clearly see on average an increase of sales during December (\autoref{fig:seasonal_plot}), which makes sense because Christmas and New years represent a season of the year with more transactions in other retail businesses. Furthermore, we highlighted February as the lowest sales month (\autoref{fig:seasonal_plot}). 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/seasonal_analysis.png}
  \caption[Analysis of seasonality]{\textbf{Analysis of seasonality}. The y-axis represents the sales aggregated by month, x-axis shows the analyzed months (from January to December), the horizontal red lines denote the sales average by month (55.5 months).}
  \label{fig:seasonal_plot}
\end{figure}

Holidays are one of the most important factors for seasonality, and the Kaggle dataset provided us with relevant holidays and special events in Ecuador (see \autoref{tab:files}). Thus, we analyzed the effects of holidays on sales (see \autoref{fig:holidays}). We observed a reasonable correlation between holidays and sales. Consequently, holidays must be incorporated on the forecast.  

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.7\textwidth]{plots/eda/holidayeffects.png}
  \caption[Holidays effect]{\textbf{Holidays effect}. Holidays and special events are represented as yellow, and purple dots, respectively.}
  \label{fig:holidays}
\end{figure}

\section[Time-series Forecast]{Time-series Forecast}
\label{sec:forecast}

The main goal of the project is to generate a reliable forecast using the sales data from Favorita stores. After the eda, we concluded that our data is not stationarity, possesses a strong seasonal effect, and holidays is a factor that should be considered in the forecast.  Therefore, the following models could be implemented:

\begin{itemize}
\item \textbf{SARIMAX}: \underline{S}easonal \underline{ARIMA} using e\underline{X}ternal data. Further, ARIMA stands for \underline{A}uto-\underline{R}egressive (p: dependence on past values), \underline{I}ntegrated (d: differencing) \underline{M}oving \underline{A}verage (q: dependence on past forecast errors). 
\item \textbf{LSTM}: \underline{L}ong-\underline{S}hort \underline{T}erm \underline{M}emory.
\item \textbf{GRU}: \underline{G}ated \underline{R}ecurrent \underline{U}nit.
\item \textbf{Facebook Prophet}: is a forecasting procedure based on a general additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.
\item \textbf{NeuralProphet}: is a hybrid forecasting framework based on PyTorch and trained with standard deep learning methods.
\item \textbf{LightGBM} (or other extreme gradient boosted method, \textit{e.g.} XGBoost): \underline{Light} \underline{G}radient-\underline{B}oosting \underline{M}achine.
\end{itemize}


Ideally, we would like to implement all of the  machine learning models mentioned above, do a benchmark taking into account: performance, computational-resources, training-time, and model-explainability; and compare their results. Nonetheless, due to time-constraints restrictions, we selected \textbf{Facebook Prophet} as our unique machine learning model to forecast the sales from Favorita stores.

 The reasons to select Facebook Prophet include: a simple and flexible model, see \autoref{eq:prophet_eq}, that takes into account multiple human scale seasonality, holidays that occur at irregular intervals, trends that are non-linear, short training time (using the \textit{L-BFGS}  optimization algorithm) , low computation-resource demands, a Python module to easily implement the model (under the hood uses Stan), model explainability, and previous experience with the model. 


\subsection[Prophet as our Forecast model]{Prophet as our Forecast model}
\label{sec:forecast_model}

Facebook Prophet or Prophet (we are going to use Prophet from now on) is an automated forecasting procedure based on a general additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.

According to Taylor \textit{et al.}, Prophet works best with time series that have strong seasonal effects (which is our case see \autoref{fig:ts-decomposition}C, and \autoref{fig:seasonal_plot}), holidays effect (\autoref{fig:holidays}), and typically handles outliers well. The additive models contains three main components: \textbf{1)} trend, \textbf{2)} seasonality, and \textbf{3)} holidays, which are combined in \autoref{eq:prophet_eq}:

\begin{align}\label{eq:prophet_eq}
y(t)= g(t) + s(t) + h(t) + \varepsilon_t  
\end{align}
\begin{align}\label{eq:trend_eq}
g(t)= (\kappa + a(t)^T\delta)t + (m + a(t)^T\gamma)
\end{align}  
\begin{align}\label{eq:seasonal_eq}
s(t)= X(t)\beta
\end{align}
\begin{align}\label{eq:holiday_eq}
h(t)= Z(t)\kappa
\end{align}
\begin{align}
\varepsilon_t \sim N(0,\sigma^2)
\end{align}
  
Where $g(t)$ is the \underline{trend} function which models non-periodic changes in the value of the time series, $s(t)$ represents \underline{seasonality} (\textit{e.g.,} weekly and yearly seasonality), and $h(t)$ represents the effects of \underline{holidays} which occur on potentially irregular schedules over one or more days. The error term $\varepsilon_t$ represents changes not explained by the model; under the assumption that $\varepsilon_t$ is normally distributed.

In \autoref{eq:trend_eq}, $\kappa$ stands for the growth rate, $\delta$ has the rate adjustments, $m$ is the offset parameter, and $\gamma_j$ is set to $-s_j\delta_j$ to make the function continuous. Next, \autoref{eq:seasonal_eq} represents seasonality which can be further extended as:

\[ s(t) = \sum_{n=1}^N \left( a_n cos\left(\frac{2\pi nt}{P}\right) + b_n sin\left( \frac{2\pi nt}{P}  \right)     \right)  \]

Let $P$ be the regular period we expect the time series to have (\textit{e.g.} $P=365.25$ and $P=7$ for yearly and weekly data, respectively). For yearly and weekly seasonality, the authors have found $N = 10$ and $N = 3$ to work well for most series problems, respectively. According to Taylor \textit{et al.}, $\beta \sim Normal(0,\sigma^2)$ to impose a smoothing prior on the
seasonality. Finally, \autoref{eq:holiday_eq} which represents the effects of holidays is extended as:

\[ Z(t) = [1(t \in  D_1) ,..., 1(t \in  D_L)]  \]

By assuming that the effects of holidays are independent. For each holiday $i$, let $D_i$ be the set of past and future dates for that holiday. And assign each holiday a parameter $\kappa_i$ which is the corresponding change in the forecast. As with seasonality, we use a prior $\kappa \sim Normal(0,\nu^2)$.


\subsubsection[Prophet Hyperparameters]{Prophet Hyperparameters}
\label{sec:hyperparameters}

Prophet possesses 5 hyperparameters which can be tuned through cross validation. The hyperparameters are: \textbf{1)} \textit{changepoint prior scale}, \textbf{2)} \textit{seasonality prior scale} , \textbf{3)} \textit{holidays prior scale} , \textbf{4)} \textit{seasonality model} , and \textbf{5)} \textit{changepoint range}  (this one not recommended to be tuned according to the Prophet developers).  

In our work, only the \textit{seasonality mode}  (either additive or multiplicative)  was tuned using cross validation (CV) with an horizon of 1 year (365 years), period of 180 days, and 730 days for training. Information about CV can be found on \href{https://facebook.github.io/prophet/docs/installation.html}{the Prophet site} under the diagnostics section, or at Taylor \textit{et al.} work. 

\subsection[Performance Metric]{Performance Metric}
\label{sec:error}

To assess the performance of our models, we selected the mean absolute percentage error (MAPE) which is defined below:

\[ MAPE= \frac{100\%}{n} \sum_{t=1}^n \left| \frac{A_t - F_t}{A_t} \right|  \]

Where $A_t$ is the actual value (true value) and $F_t$ is the forecast result (predicted value). MAPE was selected as a performance metric because of its simplicity to represent the forecast errors within a percentage scale, where the lower the better. 

\subsection[Forecast Results]{Forecast Results}
\label{sec:results}

The sales data can be organized by store (54 stores) and by store and family product (1782 combinations). Therefore, as a starting point we aggregate all sales and generate 1 forecast (see \nameref{sec:general}). Then, generate a forecast by each store (54 forecasts; see \nameref{sec:forecast-store}). Finally, we generate a forecast of each product within each store (1782 forecasts; see \nameref{sec:forecast-product}).

\subsubsection[General Sales Forecast]{General Sales Forecast}
\label{sec:general}

\autoref{fig:general-forecast} shows the forecast outcome obtained aggregating all the sales, obtaining a MAPE of 22.26\% using a half year horizon. After experimenting with different hyperparameters and conditions,  the Prophet model with the highest performance (the lowest MAPE) considers the following: 

\begin{enumerate}
\item A multiplicative seasonality mode.
\item Uses the provided holidays (holiday\_events.csv).
\item Custom regressor representing the store closing days.
\end{enumerate}
  
We observe a correct modeling of the training data, and correctly modeling the pronounced dips around Christmas and New Year (\autoref{fig:general-forecast}). Model explainability is an important factor to select a final model, in addition to performance, and computational training time. Prophet comes with an integrated explanation of the model components including the effects of holidays, weekly and yearly seasonality which helps to assess the performance of the forecast.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/forecast/forecast_prophet.png}
  \caption[General Sales Forecast]{\textbf{General Sales Forecast}. Blue lines denotes the forecast, and black dots describe the actual values. CV was implemented with 730 days leading to 4 forecasts cutoffs. Forecast (half year horizon) is represented after the red dashed line. }
  \label{fig:general-forecast}
\end{figure}

In consequence, the forecast components are displayed in \autoref{fig:forecast-components}. A sales increase is reported on the holiday component (\autoref{fig:forecast-components}A), which is expected in the retail business. In terms of weekly and yearly seasonality, Saturday and Sunday; and December are the periods with the highest sales, respectively (\autoref{fig:forecast-components}B and C).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.8\textwidth]{plots/forecast/forecast_prophet-components.png}
  \caption[General Forecast Components]{\textbf{General Forecast Components}. The components of the final Prophet model, which considers a multiplicative seasonality, holidays, and a custom regressor represented on \autoref{fig:general-forecast}. \textbf{(A)} The holiday component. \textbf{(B)} Weekly seasonality. \textbf{(C)} Yearly seasonality. \textbf{(D)} Regressor considering the store closing days.}
  \label{fig:forecast-components}
\end{figure}

The regressor displays the store closing days which explains the models the pronounced dips around Christmas and New Year (\autoref{fig:forecast-components}D). In conclusion, we can state that our proposed models possess a moderate error (MAPE of 22.26\%), and the forecast components are in alignment of what we would expect of the retail business.   

\subsubsection[Sales Forecast by Store]{Sales Forecast by Store}
\label{sec:forecast-store}

Our dataset contains 54 stores. Thus, one time series model was produced to each store leading to 54 different Prophet models. In this scenario, a vanilla Prophet model was fitted to each store. Obtaining a mean MAPE of 36.87\%, twelve random stores are represented by \autoref{fig:forecast-store}, where the orange lines represent the sales forecast by store. For better figure representation the stores are displayed from 2017-05 to 2017-09 to highlight the last dates. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/forecast/forecast_bystore.png}
  \caption[Forecast by Store]{\textbf{Forecast by Store}. The x-axis represents the dates from 2017-05 to 2017-09, the y-axis shows the sales for each store. The blue and orange lines display the store sales and forecast, respectively.}
  \label{fig:forecast-store}
\end{figure}

As expected, we obtained a higher error compared to the general sales forecast (36.87\% vs. 22.26\%). The reasons are: each store has a different sales pattern, and each Prophet model should be tuned for each store. In addition, a data normalization should be implemented to reduce the variance, and achieve a lower MAPE. Nonetheless some store predictions are reasonable, for instance for stores: 1, 3, 6, and 10 (\autoref{fig:forecast-store}). In contrast, for stores: 7, 8, and 11 the forecasts can be considered as non-correct (\autoref{fig:forecast-store}). Overall, 88\% of the forecast can be considered as correct with a mean MAPE of 25.96\% (see all the forecast plots at the \href{https://github.com/razielar/forecasting_retail-store}{GitHub repository}). Considering the complexity of time series forecasting and the different sales patterns for each store, we can label our results as satisfactory.

\subsubsection[Sales Forecast by store and product]{Sales Forecast by store and product}
\label{sec:forecast-product}

In this scenario, one vanilla Prophet model was fitted for each product for each store producing a total of 1782 forecasts. As we can see on \autoref{fig:forecast-store-product}, each product for each store presents a completely different sales behavior leading to a more complex forecast problem. On average, we obtained a MAPE of 61.02\%. Consequently, we are not able to suggest to rely on our predictions following this approach. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/forecast/forecast_bystore_product.png}
  \caption[Forecast by Store and Product]{\textbf{Forecast by Store and Product}. The x-axis represents the dates from 2017-05 to 2017-09, the y-axis shows the sales for each store and product. The different color lines represent each sales product.}
  \label{fig:forecast-store-product}
\end{figure}

\section[Recommendations]{Recommendations}
\label{sec:recommendations}





