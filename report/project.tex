\section[Main objective]{Main objective}
\label{sec:main_objective}

The \textbf{main object} of the project is to:

\begin{itemize}
\item Generate a reliable \textbf{time series forecast} based on sales from Favorita stores. 
\end{itemize}

Favorita stores is a  grocery retail store based in Ecuador. Producing an accurate forecast could lead to decreased food waste related to overstocking and improve customer satisfaction. All the code can be found in the following \href{https://github.com/razielar/forecasting_retail-store}{GitHub repository}, further information can be found in \nameref{sec:supp_info}.

\clearpage

\section[Data description]{Data description}
\label{sec:data_description}

The dataset comes from \textbf{Kaggle}, named: \href{https://www.kaggle.com/competitions/store-sales-time-series-forecasting/data}{Store Sales - Time Series Forecasting}. The Kaggle dataset contains sales data from \textit{Corporaci√≥n Favorita}, a large Ecuadorian-based grocery retailer. Kaggle provides you with 7 diffeent files (see \autoref{tab:files} for more details). 

\begin{table}[!htb]
  \caption[Kaggle files description]{\textbf{Kaggle files description}. Files are alphabetically sorted.}
  \begin{scriptsize}
    \begin{tabulary}{0.65\linewidth}{ccl}
      \textbf{Number} & \textbf{File Name} & \textbf{Description} \\ \hline
      1 & holiday\_events.csv & Relevant holidays in Ecuador  \\
      2 & oil.csv & Oil prices from 2013 to 2017  \\
      3 & sample\_submission.csv & submission example  \\
      4 & stores.csv & Stores metadata  \\
      5 & test.csv & Stores and family products  \\
      6 & train.csv & Sales by store and product-family from 2013-01 to 2017-08  \\
      7 & transactions.csv & Number of transactions by store  \\
    \end{tabulary}
  \end{scriptsize}
  \label{tab:files}
\end{table}

The training data represents 99\% of the data, including dates from 2013-01-01 to 2017-08-16 (55.5 months), 54 stores placed in different cities within Ecuador, and 33 family-products (see \autoref{fig:store_summary}). The testing data includes dates from 2017-08-16 to 2017-08-31 (15 days).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/data_description/stores_summary.png}
  \caption[Summary of the training dataset]{\textbf{Summary of the training dataset}. The cluster information denotes similarity between stores.}
  \label{fig:store_summary}
\end{figure}

\section[Data exploration and data cleaning]{Data exploration and data cleaning}
\label{sec:eda}

Performing an exploratory data analysis (eda) of the sales from the training dataset, we can observe that grocery I, beverages, and produce are the top 3 most consumed products (see \autoref{fig:eda}) . Additionally, store type A, and cluster 5 are the most frequent among their classification (\autoref{fig:eda}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/sale_analysis.png}
  \caption[EDA of sales]{\textbf{EDA of sales}. The plot describes the sales by product, store type, and per cluster (left, right, and below, respectively). Darker blue represents higher sales. }
  \label{fig:eda}
\end{figure}


The sales represented by \autoref{fig:target} shows low-peaks at the end of each year, which is explained because the stores are closed at New years. Moreover, we can observe a pattern at each year, with increased sales at the end of the year which overlaps with the Christmas eves, suggesting a seasonal pattern. These patterns are highlighted after smoothing the sales data  with a 7 days moving average (the continuous red lines from the \autoref{fig:target}).

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/target-time-series-plot-DF-testProc.png}
  \caption[Target time series data]{\textbf{Target time series data}. The y-axis represents the training sales from 2013-01-01 to 2017-08-15, and the x-axis shows the time in days (1,087 days). The sales were aggregated by all stores and products (see \autoref{fig:store_summary}). The raw data, and smoothed data (7 days moving average) are denoted by the continuous blue, and red lines, respectively.}
  \label{fig:target}
\end{figure}

\subsection[Time series Decomposition]{Time series Decomposition}
\label{sec:decomposition}

Time series are defined as a sequence of data organized in time order. The key components of time-series are: trend, seasonality, and residuals, displayed in \autoref{fig:ts-decomposition}B, \autoref{fig:ts-decomposition}C, and \autoref{fig:ts-decomposition}D, respectively. Trend is the long-term direction of the time series, seasonality describes the periodic behavior such as holidays, and residuals represent the irregular fluctuations that we are not able to predict using the trend and/or seasonality. 

In our target time series, we observe an upward trend, with a clear seasonality factor (this will be further analyzed below), and the residuals do not follow a random pattern. These insights will be used to select an adequate forecast model. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.95\textwidth]{plots/eda/timseries_decomposition.png}
  \caption[Time series decomposition]{\textbf{Time series decomposition}. \textbf{(A)} Raw sales time series (sames as \autoref{fig:target}). \textbf{(B)} Trend of sales. \textbf{(C)} Seasonality of sales. \textbf{(D)} Residuals of sales. }
  \label{fig:ts-decomposition}
\end{figure}

\subsection[Stationarity and seasonality analyses]{Stationarity and seasonality analyses}
\label{sec:st_s_analysis}


The stationary and seasonality are relevant components to select the adequate machine learning model (such as ARIMA, SARIMA, etc.) to generate reliable forecasting. A stationary time series is defined, if their statistical properties such as mean, and variance are all constant, and independent of time.

In consequence, we implemented a \textit{Dickey-Fuller test} to assess stationarity. We obtained a \textit{p-value} of 0.09, using a significance level of 0.05, we can reject the null hypothesis (the series is stationary) concluding that our series is non-stationarity.

In terms of seasonality, we aggregate the sales for each month and we can clearly see on average an increase of sales during December (\autoref{fig:seasonal_plot}), which makes sense because Christmas and New years represent a season of the year with more transactions in other retail businesses. Furthermore, we highlighted February as the lowest sales month. 

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.9\textwidth]{plots/eda/seasonal_analysis.png}
  \caption[Analysis of seasonality]{\textbf{Analysis of seasonality}. The y-axis represents the sales aggregated by month, x-axis shows the analyzed months (from January to December), the horizontal red lines denote the sales average by month (55.5 months).}
  \label{fig:seasonal_plot}
\end{figure}

Holidays are one of the most important factors for seasonality, and the Kaggle dataset provided us with relevant holidays and special events in Ecuador (see \autoref{tab:files}). Thus, we analyzed the effects of holidays on sales (see \autoref{fig:holidays}). We observed a reasonable correlation between holidays and sales. Consequently, holidays must be incorporated on the forecast.  

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.7\textwidth]{plots/eda/holidayeffects.png}
  \caption[Holidays effect]{\textbf{Holidays effect}. Holidays and special events are represented as yellow, and purple dots, respectively.}
  \label{fig:holidays}
\end{figure}

\section[Time-series Forecast]{Time-series Forecast}
\label{sec:forecast}

The main goal of the project is to generate a reliable forecast using the sales data from Favorita stores. After the eda, we concluded that our data is not stationarity, possesses a strong seasonal effect, and holidays is a factor that should be considered in the forecast.  Therefore, the following models could be implemented:

\begin{itemize}
\item \textbf{SARIMAX}: \underline{S}easonal \underline{ARIMA} using e\underline{X}ternal data. Further, ARIMA stands for \underline{A}uto-\underline{R}egressive (p: dependence on past values), \underline{I}ntegrated (d: differencing) \underline{M}oving \underline{A}verage (q: dependence on past forecast errors). 
\item \textbf{LSTM}: \underline{L}ong-\underline{S}hort \underline{T}erm \underline{M}emory.
\item \textbf{GRU}: \underline{G}ated \underline{R}ecurrent \underline{U}nit.
\item \textbf{Facebook Prophet}: is a forecasting procedure based on a general additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.
\item \textbf{NeuralProphet}: is a hybrid forecasting framework based on PyTorch and trained with standard deep learning methods.
\item \textbf{LightGBM} (or other extreme gradient boosted method, \textit{e.g.} XGBoost): \underline{Light} \underline{G}radient-\underline{B}oosting \underline{M}achine.
\end{itemize}


Ideally, we would like to implement all of the  machine learning models mentioned above, do a benchmark taking into account: performance, computational-resources, training-time, and model-explainability; and compare their results. Nonetheless, due to time-constraints restrictions, we selected \textbf{Facebook Prophet} as our unique machine learning model to forecast the sales from Favorita stores.

 The reasons to select Facebook Prophet include: a simple and flexible model (further described in below) that takes into account multiple human scale seasonality, holidays that occur at irregular intervals, trends that are non-linear, short training time, low computation-resource demands, a Python module to easily implement the model (under the hood uses Stan), and previous experience with the model. 


\subsection[Prophet as our Forecast model]{Prophet as our Forecast model}
\label{sec:forecast_model}

Facebook Prophet or Prophet (we are going to use Prophet from now on) is an automated forecasting procedure based on a general additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.

According to Taylor \textit{et al.}, Prophet works best with time series that have strong seasonal effects (which is our case see \autoref{fig:ts-decomposition}C, and \autoref{fig:seasonal_plot}), holidays effect (\autoref{fig:holidays}), and typically handles outliers well. The additive models contains three main components: \textbf{1)} trend, \textbf{2)} seasonality, and \textbf{3)} holidays, which are combined in equation (1) :

\begin{align}
y(t)= g(t) + s(t) + h(t) + \varepsilon_t  
\end{align}
\begin{align}
g(t)= (\kappa + a(t)^T\delta)t + (m + a(t)^T\gamma)
\end{align}  
\begin{align}
s(t)= X(t)\beta
\end{align}
\begin{align}
h(t)= Z(t)\kappa
\end{align}
\begin{align}
\varepsilon_t \sim N(0,\sigma^2)
\end{align}
  
Where $g(t)$ is the \underline{trend} function which models non-periodic changes in the value of the time series, $s(t)$ represents seasonality (\textit{e.g.,} weekly and yearly seasonality), and $h(t)$ represents the effects of \underline{holidays} which occur on potentially irregular schedules over one or more days. The error term $\varepsilon_t$ represents changes not explained by the model; under the assumption that $\varepsilon_t$ is normally distributed.

Next, equation (3) which represents seasonality can be further extended as:

\[ X(t) = \left[ cos\left(\frac{2\pi(1)t}{365.25}\right) ,..., sin\left( \frac{2\pi(10)t}{365.25}  \right)     \right]  \]

Finally, equation (4) which represents the effects of holidays is extended as:

\[ Z(t) = [1(t \in  D_1) ,..., 1(t \in  D_L)]  \]


\subsubsection[Prophet Hyperparameters]{Prophet Hyperparameters}
\label{sec:hyperparameters}

Prophet possesses 5 hyperparameters which can be tuned through cross validation. The hyperparameters are: \textbf{1)} \textit{changepoint prior scale}, \textbf{2)} \textit{seasonality prior scale} , \textbf{3)} \textit{holidays prior scale} , \textbf{4)} \textit{seasonality model} , and \textbf{5)} \textit{changepoint range}  (this one not recommended to be tuned according to the Prophet developers).  

In our work, only the \textit{seasonality mode}  (either additive or multiplicative)  was tuned using cross validation with an horizon of 1 year (365 years), period of 180 days, and 730 days for training. Information about cross validation can be found on \href{https://facebook.github.io/prophet/docs/installation.html}{the Prophet site} under the diagnostics section, or at Taylor \textit{et al.} work. 

\subsection[Performance Metric]{Performance Metric}
\label{sec:error}

To assess the performance of our models, we selected the mean absolute percentage error (MAPE) which is defined below:

\[ MAPE= \frac{100\%}{n} \sum_{t=1}^n \mid \frac{A_t - F_t}{A_t} \mid  \]

Where $A_t$ is the actual value (true value) and $F_t$ is the forecast result (predicted value). MAPE was selected as a performance metric because of its simplicity to represent the forecast errors within a percentage scale, where the lower the better. 

\subsection[Forecast results]{Forecast results}
\label{sec:results}






